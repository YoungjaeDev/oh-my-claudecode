# Computer Vision Model Architecture Diagrams

ASCII 다이어그램으로 표현한 주요 컴퓨터 비전 모델 아키텍처 및 핵심 개념

## 목차
1. [Detection Models](#detection-models)
2. [Segmentation Models](#segmentation-models)
3. [Vision-Language Models](#vision-language-models)
4. [Core Concepts](#core-concepts)
5. [Data Flow Diagrams](#data-flow-diagrams)

---

## Detection Models

### YOLO Architecture

```
Input Image (640×640×3)
       │
       ▼
┌──────────────────────────────────────────────────┐
│              Backbone (Feature Extraction)        │
│                   CSPDarknet / C2f                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │   P3     │  │   P4     │  │   P5     │       │
│  │ 80×80×256│  │ 40×40×512│  │ 20×20×1024│      │
│  └──────────┘  └──────────┘  └──────────┘       │
│  (작은 객체)    (중간 객체)    (큰 객체)          │
└────────┬──────────────┬──────────────┬───────────┘
         │              │              │
         ▼              ▼              ▼
┌──────────────────────────────────────────────────┐
│              Neck (Feature Fusion)                │
│           PANet / FPN (Path Aggregation)          │
│                                                   │
│  Top-Down  │           │  Bottom-Up               │
│  Path      ▼           ▼  Path                    │
│        ┌──────┐   ┌──────┐                       │
│        │ Fuse │◄──┤ Fuse │                       │
│        └──────┘   └──────┘                       │
│                                                   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │   S      │  │   M      │  │   L      │       │
│  │  Small   │  │  Medium  │  │  Large   │       │
│  └──────────┘  └──────────┘  └──────────┘       │
└────────┬──────────────┬──────────────┬───────────┘
         │              │              │
         ▼              ▼              ▼
┌──────────────────────────────────────────────────┐
│            Head (Prediction Layers)               │
│                                                   │
│  ┌─────────────┐  ┌─────────────┐               │
│  │ Bounding    │  │ Class       │               │
│  │ Box Coords  │  │ Scores      │               │
│  │ (x,y,w,h)   │  │ (conf×prob) │               │
│  └─────────────┘  └─────────────┘               │
│                                                   │
│  Output: [N, 4+C]                                │
│  N = number of detections                        │
│  C = number of classes                           │
└──────────────────────────────────────────────────┘
```

**Key Components (주요 구성 요소):**
- **Backbone**: 이미지에서 계층적 특징 추출
- **Neck**: 다양한 스케일의 특징 융합
- **Head**: 최종 객체 탐지 결과 예측

---

### RT-DETR Architecture (Transformer-based)

```
Input Image (640×640×3)
       │
       ▼
┌──────────────────────────────────────────────────┐
│           Backbone (ResNet / HGNetv2)             │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │   C3     │  │   C4     │  │   C5     │       │
│  │  80×80   │  │  40×40   │  │  20×20   │       │
│  └──────────┘  └──────────┘  └──────────┘       │
└────────┬──────────────┬──────────────┬───────────┘
         │              │              │
         ▼              ▼              ▼
┌──────────────────────────────────────────────────┐
│        Hybrid Encoder (CNN + Transformer)         │
│                                                   │
│  ┌─────────────────────────────────────┐         │
│  │  AIFI (Attention-based Fusion)      │         │
│  │  ┌─────┐   ┌─────┐   ┌─────┐       │         │
│  │  │ Self│──▶│Multi│──▶│Feed │       │         │
│  │  │Attn │   │Head │   │Fwd  │       │         │
│  │  └─────┘   └─────┘   └─────┘       │         │
│  └─────────────────────────────────────┘         │
│                                                   │
│  Multi-scale Features → Flattened Sequence       │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│      Transformer Decoder (with Queries)           │
│                                                   │
│  ┌─────────────┐         ┌──────────────┐       │
│  │  Object     │────────▶│ Self-        │       │
│  │  Queries    │         │ Attention    │       │
│  │  (300 box)  │         └──────────────┘       │
│  └─────────────┘                │               │
│                                  ▼               │
│                         ┌──────────────┐        │
│                         │ Cross-       │        │
│  Encoder Features ─────▶│ Attention    │        │
│                         └──────────────┘        │
│                                  │               │
│                                  ▼               │
│                         ┌──────────────┐        │
│                         │ FFN          │        │
│                         └──────────────┘        │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│            Prediction Heads                       │
│  ┌─────────────┐  ┌─────────────┐               │
│  │ Box Head    │  │ Class Head  │               │
│  │ (x,y,w,h)   │  │ (C classes) │               │
│  └─────────────┘  └─────────────┘               │
│                                                   │
│  No NMS Required! (직접 최종 예측 출력)           │
└──────────────────────────────────────────────────┘
```

**Key Differences vs YOLO (YOLO와의 차이점):**
- No anchor boxes (앵커 박스 불필요)
- No NMS post-processing (NMS 후처리 불필요)
- End-to-end differentiable (완전 미분 가능)
- Better for small objects (작은 객체 탐지 우수)

---

## Segmentation Models

### SAM (Segment Anything Model) Architecture

```
┌──────────────────────────────────────────────────┐
│               SAM Architecture                    │
└──────────────────────────────────────────────────┘

Input Image (1024×1024×3)
       │
       ▼
┌──────────────────────────────────────────────────┐
│         Image Encoder (ViT-H/L/B)                 │
│         Vision Transformer Backbone               │
│                                                   │
│  ┌─────────────────────────────────────────┐    │
│  │  Patch Embedding (16×16 patches)        │    │
│  │              ▼                           │    │
│  │  ┌────────────────────────────────┐     │    │
│  │  │  Transformer Blocks (32 layers)│     │    │
│  │  │  ┌─────┐  ┌─────┐  ┌─────┐    │     │    │
│  │  │  │MSA  │─▶│MLP  │─▶│Norm │    │     │    │
│  │  │  └─────┘  └─────┘  └─────┘    │     │    │
│  │  │      ×32 repetitions           │     │    │
│  │  └────────────────────────────────┘     │    │
│  └─────────────────────────────────────────┘    │
│                                                   │
│  Output: Image Embedding (256×64×64)             │
└────────────────────┬─────────────────────────────┘
                     │
      ┌──────────────┴──────────────┐
      │                             │
      ▼                             ▼
┌──────────────┐          ┌──────────────────────┐
│   Prompt     │          │  Image Embedding     │
│   Encoder    │          │  (from above)        │
│              │          └──────────────────────┘
│  ┌────────┐  │                    │
│  │ Points │──┤                    │
│  └────────┘  │                    │
│  ┌────────┐  │                    │
│  │ Boxes  │──┤                    │
│  └────────┘  │                    │
│  ┌────────┐  │                    │
│  │ Masks  │──┤                    │
│  └────────┘  │                    │
│  ┌────────┐  │                    │
│  │ Text   │──┤                    │
│  └────────┘  │                    │
│              │                    │
│  Sparse/Dense│                    │
│  Embeddings  │                    │
└──────┬───────┘                    │
       │                            │
       │                            │
       └────────────┬───────────────┘
                    ▼
┌──────────────────────────────────────────────────┐
│           Mask Decoder                            │
│        (Lightweight Transformer)                  │
│                                                   │
│  ┌─────────────────────────────────────────┐    │
│  │  Two-way Transformer Blocks (2 layers)  │    │
│  │                                          │    │
│  │  Prompt ◄──────────┐                    │    │
│  │  Tokens            │                    │    │
│  │    │               │                    │    │
│  │    ▼               │                    │    │
│  │  ┌─────────┐  ┌─────────┐             │    │
│  │  │  Self   │  │ Cross   │             │    │
│  │  │  Attn   │  │ Attn    │             │    │
│  │  └─────────┘  └─────────┘             │    │
│  │    │               ▲                    │    │
│  │    └───────────────┘                    │    │
│  │                                          │    │
│  │  Image Embedding                        │    │
│  └─────────────────────────────────────────┘    │
│                    ▼                             │
│  ┌─────────────────────────────────────────┐    │
│  │  MLP (Multi-Layer Perceptron)           │    │
│  │  Upsampling layers (4×)                 │    │
│  └─────────────────────────────────────────┘    │
└────────────────────┬─────────────────────────────┘
                     ▼
┌──────────────────────────────────────────────────┐
│            Output Masks (3 candidates)            │
│                                                   │
│  ┌─────────────┐  ┌─────────────┐  ┌──────────┐│
│  │   Mask 1    │  │   Mask 2    │  │  Mask 3  ││
│  │  (whole)    │  │   (part)    │  │ (subpart)││
│  │  IoU: 0.95  │  │  IoU: 0.88  │  │ IoU: 0.72││
│  └─────────────┘  └─────────────┘  └──────────┘│
│                                                   │
│  Automatically ranked by predicted IoU            │
└──────────────────────────────────────────────────┘
```

**Prompting Examples (프롬프트 예시):**
```
Points:         Boxes:          Masks:
  ┌─────┐       ┌─────────┐     ┌─────────┐
  │  •  │       │ ┌─────┐ │     │ ░░░░░   │
  │     │       │ │     │ │     │ ░░░░░░  │
  └─────┘       │ └─────┘ │     │  ░░░░░  │
                └─────────┘     └─────────┘
```

---

### SAM 2 Architecture (Video Segmentation)

```
┌──────────────────────────────────────────────────┐
│              SAM 2 Architecture                   │
│           (Segment Anything in Video)             │
└──────────────────────────────────────────────────┘

Frame t-2      Frame t-1      Frame t      Frame t+1
   │              │              │              │
   ▼              ▼              ▼              ▼
┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐
│ Image  │    │ Image  │    │ Image  │    │ Image  │
│Encoder │    │Encoder │    │Encoder │    │Encoder │
│ (ViT)  │    │ (ViT)  │    │ (ViT)  │    │ (ViT)  │
└────┬───┘    └────┬───┘    └────┬───┘    └────┬───┘
     │             │             │             │
     └─────────────┴─────────────┴─────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│            Memory Attention Module                │
│                                                   │
│  ┌──────────────────────────────────────────┐   │
│  │         Memory Bank                      │   │
│  │  ┌────────┬────────┬────────┬────────┐  │   │
│  │  │Frame   │Frame   │Frame   │Frame   │  │   │
│  │  │t-2     │t-1     │t       │t+1     │  │   │
│  │  │Features│Features│Features│Features│  │   │
│  │  └────────┴────────┴────────┴────────┘  │   │
│  │                                          │   │
│  │  + Object Pointers (객체 추적 포인터)     │   │
│  │  + Mask Predictions (이전 마스크)         │   │
│  └──────────────────────────────────────────┘   │
│                   │                              │
│                   ▼                              │
│  ┌──────────────────────────────────────────┐   │
│  │      Cross-Attention with Memory         │   │
│  │                                          │   │
│  │  Current Frame ──┐                      │   │
│  │                  ▼                      │   │
│  │              ┌────────┐                 │   │
│  │              │ Cross  │                 │   │
│  │  Memory Bank─┤  Attn  │                 │   │
│  │              └────────┘                 │   │
│  └──────────────────────────────────────────┘   │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│           Mask Decoder (per frame)                │
│                                                   │
│  ┌────────────┐  ┌────────────┐                 │
│  │   Prompt   │  │  Memory-   │                 │
│  │  Encoder   │  │  Enhanced  │                 │
│  │            │  │  Features  │                 │
│  └────────────┘  └────────────┘                 │
│         │              │                         │
│         └──────┬───────┘                         │
│                ▼                                 │
│      ┌──────────────────┐                       │
│      │  Decoder Blocks  │                       │
│      └──────────────────┘                       │
│                │                                 │
│                ▼                                 │
│      ┌──────────────────┐                       │
│      │  Upsampling MLP  │                       │
│      └──────────────────┘                       │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
         ┌───────────────────────┐
         │  Output Mask + Update │
         │  Memory Bank          │
         └───────────────────────┘
              │
              ▼
         (Next Frame)
```

**Key Features (주요 기능):**
- **Memory Bank**: 이전 프레임 정보 저장
- **Streaming**: 실시간 비디오 처리 가능
- **Temporal Consistency**: 시간적 일관성 유지
- **Occlusion Handling**: 가려진 객체 추적

**Processing Flow (처리 흐름):**
```
Initialize with prompt (첫 프레임에서 객체 선택)
      │
      ▼
┌──────────────────┐
│ Process Frame t  │──┐
└──────────────────┘  │
      │               │
      ▼               │
┌──────────────────┐  │
│ Generate Mask    │  │
└──────────────────┘  │
      │               │
      ▼               │
┌──────────────────┐  │
│ Update Memory    │  │
└──────────────────┘  │
      │               │
      └───────────────┘
      (Loop for all frames)
```

---

### YOLO-Seg Architecture

```
┌──────────────────────────────────────────────────┐
│          YOLO-Seg (Instance Segmentation)         │
└──────────────────────────────────────────────────┘

Input Image (640×640×3)
       │
       ▼
┌──────────────────────────────────────────────────┐
│         Backbone + Neck (Same as YOLO)            │
│              (Feature Extraction)                 │
└────────────────────┬─────────────────────────────┘
                     │
         ┌───────────┴───────────┐
         │                       │
         ▼                       ▼
┌──────────────────┐   ┌──────────────────────────┐
│ Detection Head   │   │  Segmentation Head       │
│                  │   │                          │
│  ┌──────────┐   │   │  ┌──────────────────┐   │
│  │ Boxes    │   │   │  │ Mask Prototypes  │   │
│  │ (x,y,w,h)│   │   │  │   (32 channels)  │   │
│  └──────────┘   │   │  └──────────────────┘   │
│  ┌──────────┐   │   │           │              │
│  │ Classes  │   │   │           ▼              │
│  │ (scores) │   │   │  ┌──────────────────┐   │
│  └──────────┘   │   │  │ Mask Coefficients│   │
│  ┌──────────┐   │   │  │   (per box)      │   │
│  │ Mask     │───┼───┼─▶│   [N × 32]       │   │
│  │ Coeffs   │   │   │  └──────────────────┘   │
│  └──────────┘   │   │                          │
└──────────────────┘   └──────────────────────────┘
         │                       │
         │                       │
         └───────────┬───────────┘
                     ▼
┌──────────────────────────────────────────────────┐
│           Mask Assembly Process                   │
│                                                   │
│  ┌────────────┐      ┌──────────────┐           │
│  │  Mask      │      │   Mask       │           │
│  │ Prototypes │  ×   │ Coefficients │           │
│  │  [H×W×32]  │      │   [N×32]     │           │
│  └────────────┘      └──────────────┘           │
│         │                   │                    │
│         └─────────┬─────────┘                    │
│                   ▼                              │
│         ┌──────────────────┐                    │
│         │  Matrix Multiply  │                    │
│         │   (Linear Combo)  │                    │
│         └──────────────────┘                    │
│                   │                              │
│                   ▼                              │
│         ┌──────────────────┐                    │
│         │  Crop with Boxes  │                    │
│         └──────────────────┘                    │
│                   │                              │
│                   ▼                              │
│         ┌──────────────────┐                    │
│         │  Sigmoid + Binary │                    │
│         └──────────────────┘                    │
└────────────────────┬─────────────────────────────┘
                     ▼
         ┌───────────────────────┐
         │  Instance Masks       │
         │  (one per detection)  │
         └───────────────────────┘
```

**Mask Generation Formula (마스크 생성 공식):**
```
Mask = Sigmoid(Prototypes × Coefficients^T)

Where:
  Prototypes:   [H×W×32]  (공유 특징 맵)
  Coefficients: [N×32]    (각 객체별 계수)
  Output:       [N×H×W]   (N개 객체의 마스크)
```

---

## Vision-Language Models

### Florence-2 Architecture

```
┌──────────────────────────────────────────────────┐
│         Florence-2 (Multi-task VLM)               │
└──────────────────────────────────────────────────┘

Input Image              Task Prompt (text)
   (Image)                   (String)
      │                         │
      ▼                         ▼
┌──────────────┐        ┌──────────────┐
│   Vision     │        │    Text      │
│   Encoder    │        │   Tokenizer  │
│   (DaViT)    │        └──────┬───────┘
│              │               │
│ ┌──────────┐ │               │
│ │  Patch   │ │               │
│ │  Embed   │ │               │
│ └────┬─────┘ │               │
│      ▼       │               │
│ ┌──────────┐ │               │
│ │ViT Blocks│ │               │
│ │  (12×)   │ │               │
│ └────┬─────┘ │               │
│      ▼       │               │
│ ┌──────────┐ │               │
│ │ Multi-   │ │               │
│ │ Scale    │ │               │
│ │ Features │ │               │
│ └────┬─────┘ │               │
└──────┼───────┘               │
       │                       │
       ▼                       │
┌──────────────┐               │
│  Vision-to-  │               │
│  Language    │               │
│  Projector   │               │
│  (Linear)    │               │
└──────┬───────┘               │
       │                       │
       └───────────┬───────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│        Language Model (Transformer Decoder)       │
│                    BART-based                     │
│                                                   │
│  ┌────────────────────────────────────────────┐ │
│  │  Encoder-Decoder Attention Layers          │ │
│  │                                            │ │
│  │  Visual     Task                          │ │
│  │  Tokens  +  Prompt  ──▶ [Transformer]     │ │
│  │                         Layers (6×)        │ │
│  │                              │             │ │
│  │                              ▼             │ │
│  │                       ┌──────────┐        │ │
│  │                       │ LM Head  │        │ │
│  │                       └──────────┘        │ │
│  └────────────────────────────────────────────┘ │
└────────────────────┬─────────────────────────────┘
                     ▼
┌──────────────────────────────────────────────────┐
│           Task-Specific Outputs                   │
│                                                   │
│  Task Prompt              Output Format          │
│  ────────────────────────────────────────────    │
│  "<OD>"                →  Boxes + Labels         │
│  "<CAPTION>"           →  Text description       │
│  "<OCR>"               →  Text + Locations       │
│  "<REGION_PROPOSAL>"   →  Bounding boxes         │
│  "<DENSE_REGION>"      →  Boxes + Captions       │
│  "<SEGMENT>"           →  Masks (via bbox)       │
│  "<PHRASE_GROUNDING>"  →  Text → Boxes           │
└──────────────────────────────────────────────────┘
```

**Multi-Task Prompting (멀티태스크 프롬프팅):**
```
┌─────────────────────────────────────────┐
│         Task Prompt Examples             │
├─────────────────────────────────────────┤
│                                         │
│  Object Detection:                      │
│    Input:  "<OD>"                       │
│    Output: "<loc_0><text>dog</text>     │
│            <loc_1><text>cat</text>"     │
│                                         │
│  Caption:                               │
│    Input:  "<CAPTION>"                  │
│    Output: "A dog and cat playing       │
│             in the garden"              │
│                                         │
│  Phrase Grounding:                      │
│    Input:  "<GROUNDING>the red ball"    │
│    Output: "<loc_42>"                   │
│                                         │
└─────────────────────────────────────────┘
```

---

### Qwen2.5-VL Architecture

```
┌──────────────────────────────────────────────────┐
│          Qwen2.5-VL Architecture                  │
│      (Large-scale Vision-Language Model)          │
└──────────────────────────────────────────────────┘

Image Input                  Text Input
(Any Resolution)             (Question/Task)
      │                            │
      ▼                            │
┌──────────────────┐               │
│  Dynamic         │               │
│  Preprocessor    │               │
│                  │               │
│  ┌────────────┐  │               │
│  │ Resize to  │  │               │
│  │ Keep Ratio │  │               │
│  └────────────┘  │               │
│        │         │               │
│        ▼         │               │
│  ┌────────────┐  │               │
│  │ Patch      │  │               │
│  │ Extraction │  │               │
│  │ (Variable) │  │               │
│  └────────────┘  │               │
└────────┬─────────┘               │
         │                         │
         ▼                         │
┌──────────────────┐               │
│  Vision          │               │
│  Encoder         │               │
│  (ViT-based)     │               │
│                  │               │
│  ┌────────────┐  │               │
│  │ Position   │  │               │
│  │ Embedding  │  │               │
│  │ (2D Aware) │  │               │
│  └────────────┘  │               │
│        │         │               │
│        ▼         │               │
│  ┌────────────┐  │               │
│  │Transformer │  │               │
│  │ Blocks     │  │               │
│  │  (24-48×)  │  │               │
│  └────────────┘  │               │
└────────┬─────────┘               │
         │                         │
         ▼                         │
┌──────────────────┐               │
│  Vision-Language │               │
│  Adapter         │               │
│  (MLP Projection)│               │
└────────┬─────────┘               │
         │                         │
         └────────────┬────────────┘
                      ▼
┌──────────────────────────────────────────────────┐
│        Qwen2.5 Language Model (Decoder)           │
│                  (7B-72B params)                  │
│                                                   │
│  ┌────────────────────────────────────────────┐ │
│  │  Unified Token Sequence                    │ │
│  │                                            │ │
│  │  [IMG_TOK_1] ... [IMG_TOK_N] [TEXT_TOK]   │ │
│  │      ▼                           ▼         │ │
│  │  ┌──────────────────────────────────────┐ │ │
│  │  │  Transformer Decoder Layers          │ │ │
│  │  │                                      │ │ │
│  │  │  ┌─────────────┐  ┌─────────────┐  │ │ │
│  │  │  │Self-Attn    │  │   FFN       │  │ │ │
│  │  │  │(Causal Mask)│  │   (GeLU)    │  │ │ │
│  │  │  └─────────────┘  └─────────────┘  │ │ │
│  │  │        ×40-80 layers               │ │ │
│  │  └──────────────────────────────────────┘ │ │
│  └────────────────────────────────────────────┘ │
└────────────────────┬─────────────────────────────┘
                     ▼
┌──────────────────────────────────────────────────┐
│              Text Generation                      │
│                                                   │
│  ┌────────────────────────────────────────────┐ │
│  │  LM Head (Vocabulary Projection)           │ │
│  │           ▼                                │ │
│  │  Generated Text (autoregressive)          │ │
│  │                                            │ │
│  │  "The image shows a golden retriever      │ │
│  │   playing fetch in a park. The dog is...  │ │
│  └────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────┘
```

**Dynamic Resolution Handling (동적 해상도 처리):**
```
Original Image
┌────────────────────┐
│  1920 × 1080       │
│  (16:9 ratio)      │
└────────────────────┘
         │
         ▼
┌────────────────────┐
│  Resize to fit     │
│  Max 1024×1024     │
│  Keep aspect ratio │
└────────────────────┘
         │
         ▼
┌────────────────────┐
│  1024 × 576        │
│  (maintains 16:9)  │
└────────────────────┘
         │
         ▼
┌────────────────────┐
│  Patch: 64 × 36    │
│  tokens (2304 vis) │
└────────────────────┘
```

---

### PaliGemma Architecture

```
┌──────────────────────────────────────────────────┐
│            PaliGemma Architecture                 │
│         (Google's VLM: SigLIP + Gemma)            │
└──────────────────────────────────────────────────┘

Image (224×224 or 448×448)      Text Prompt
         │                           │
         ▼                           │
┌──────────────────┐                 │
│   SigLIP Vision  │                 │
│     Encoder      │                 │
│   (ViT-So400m)   │                 │
│                  │                 │
│  ┌────────────┐  │                 │
│  │  Patch     │  │                 │
│  │  Embed     │  │                 │
│  │  (14×14)   │  │                 │
│  └────┬───────┘  │                 │
│       ▼          │                 │
│  ┌────────────┐  │                 │
│  │ Transformer│  │                 │
│  │   Layers   │  │                 │
│  │   (27×)    │  │                 │
│  └────┬───────┘  │                 │
│       ▼          │                 │
│  ┌────────────┐  │                 │
│  │  [CLS] +   │  │                 │
│  │  Patch     │  │                 │
│  │  Tokens    │  │                 │
│  └────┬───────┘  │                 │
└───────┼──────────┘                 │
        │                            │
        ▼                            │
┌──────────────────┐                 │
│  Linear          │                 │
│  Projection      │                 │
│  (ViT dim →      │                 │
│   Gemma dim)     │                 │
└────────┬─────────┘                 │
         │                           │
         │  ┌────────────────┐       │
         └─▶│  Image Tokens  │       │
            │  (256 tokens)  │       │
            └────────┬───────┘       │
                     │               │
                     └───────┬───────┘
                             ▼
┌──────────────────────────────────────────────────┐
│         Gemma 2B Language Model                   │
│         (Decoder-only Transformer)                │
│                                                   │
│  ┌────────────────────────────────────────────┐ │
│  │  Full Sequence:                            │ │
│  │  [IMG_TOK_1...IMG_TOK_256] [TEXT_TOKENS]  │ │
│  │                                            │ │
│  │         All attended together              │ │
│  │  (image tokens are part of context)       │ │
│  └────────────────────────────────────────────┘ │
│                     │                            │
│                     ▼                            │
│  ┌────────────────────────────────────────────┐ │
│  │  Gemma Decoder Blocks (18 layers)         │ │
│  │                                            │ │
│  │  ┌──────────────┐  ┌──────────────┐      │ │
│  │  │ RMSNorm      │  │  FFN (GeLU)  │      │ │
│  │  │ Self-Attn    │  │  RMSNorm     │      │ │
│  │  │ (GQA)        │  │              │      │ │
│  │  └──────────────┘  └──────────────┘      │ │
│  │       ×18 repetitions                     │ │
│  └────────────────────────────────────────────┘ │
│                     │                            │
│                     ▼                            │
│  ┌────────────────────────────────────────────┐ │
│  │        Output Head                         │ │
│  │  (Vocabulary: 256k tokens)                 │ │
│  └────────────────────────────────────────────┘ │
└────────────────────┬─────────────────────────────┘
                     ▼
         ┌───────────────────────┐
         │  Generated Response   │
         │  (Text / Coordinates) │
         └───────────────────────┘
```

**Key Features (주요 특징):**
- **SigLIP**: 대조 학습 기반 비전 인코더
- **Gemma**: 경량화된 언어 모델 (2B params)
- **Frozen Vision**: 비전 인코더는 고정, LM만 학습
- **Multi-resolution**: 224×224 (fast) or 448×448 (detailed)

---

## Core Concepts

### NMS (Non-Maximum Suppression)

```
┌──────────────────────────────────────────────────┐
│      Non-Maximum Suppression Process              │
└──────────────────────────────────────────────────┘

Step 1: Initial Detections (중복된 박스들)
┌─────────────────────────────┐
│                             │
│   ┌──────────┐              │
│   │  Box A   │              │
│   │ conf=0.9 │              │
│   └──────────┘              │
│     ┌──────────┐            │
│     │  Box B   │            │
│     │ conf=0.8 │            │
│     └──────────┘            │
│      ┌──────────┐           │
│      │  Box C   │           │
│      │ conf=0.7 │           │
│      └──────────┘           │
│                             │
└─────────────────────────────┘

Step 2: Sort by Confidence (신뢰도 정렬)
┌──────────────────────────────┐
│  Sorted List:                │
│  1. Box A (0.9) ← Select     │
│  2. Box B (0.8) ← Check IoU  │
│  3. Box C (0.7) ← Check IoU  │
└──────────────────────────────┘

Step 3: Calculate IoU with Selected Box
┌─────────────────────────────┐
│   IoU(A, B) = 0.75          │
│   IoU(A, C) = 0.45          │
│                             │
│   Threshold = 0.5           │
│                             │
│   Decision:                 │
│   - Box B: 0.75 > 0.5 → 제거 │
│   - Box C: 0.45 < 0.5 → 유지 │
└─────────────────────────────┘

Step 4: Final Result (최종 결과)
┌─────────────────────────────┐
│                             │
│   ┌──────────┐              │
│   │  Box A   │              │
│   │ conf=0.9 │              │
│   └──────────┘              │
│                             │
│      ┌──────────┐           │
│      │  Box C   │           │
│      │ conf=0.7 │           │
│      └──────────┘           │
│                             │
└─────────────────────────────┘
```

**Algorithm (알고리즘):**
```
function NMS(boxes, scores, iou_threshold):
    1. Sort boxes by score (descending)
    2. selected = []
    3. While boxes not empty:
         a. Pick box with highest score
         b. Add to selected
         c. Remove boxes with IoU > threshold
    4. Return selected

Time Complexity: O(N²) where N = number of boxes
```

---

### IoU (Intersection over Union) Calculation

```
┌──────────────────────────────────────────────────┐
│         IoU Calculation Visualization             │
└──────────────────────────────────────────────────┘

Two Bounding Boxes:
         Box A               Box B
    ┌────────────┐      ┌────────────┐
    │            │      │            │
    │    (x1,y1) │      │            │
    │      •─────┼──────┼─•          │
    │      │     │      │ │          │
    │      │ ┌───┼──────┤ │          │
    │      │ │   │ Inter│ │          │
    └──────┼─┤   └──────┼─┘          │
           │ │          │            │
           │ │          │   (x2,y2)  │
           │ └──────────┼─────•      │
           │            │            │
           └────────────┘────────────┘

Calculation Steps:

1. Find Intersection Rectangle:
   ┌───────────────────────────────────┐
   │  x1_inter = max(x1_A, x1_B)       │
   │  y1_inter = max(y1_A, y1_B)       │
   │  x2_inter = min(x2_A, x2_B)       │
   │  y2_inter = min(y2_A, y2_B)       │
   └───────────────────────────────────┘

2. Calculate Areas:
   ┌───────────────────────────────────┐
   │  inter_w = max(0, x2_inter - x1_inter)     │
   │  inter_h = max(0, y2_inter - y1_inter)     │
   │  area_inter = inter_w × inter_h            │
   │                                            │
   │  area_A = (x2_A - x1_A) × (y2_A - y1_A)   │
   │  area_B = (x2_B - x1_B) × (y2_B - y1_B)   │
   │  area_union = area_A + area_B - area_inter│
   └───────────────────────────────────┘

3. Compute IoU:
   ┌───────────────────────────────────┐
   │           area_inter              │
   │  IoU = ─────────────────          │
   │           area_union              │
   │                                   │
   │  Range: [0, 1]                    │
   │  - 0: No overlap                  │
   │  - 1: Perfect match               │
   └───────────────────────────────────┘

Examples:
┌─────────────────────────────────────────────┐
│  No Overlap:         Partial:      Perfect: │
│  ┌───┐              ┌───┐          ┌───┐   │
│  │ A │  ┌───┐       │ A┌┼─┐        │ A │   │
│  └───┘  │ B │       └──┼┘B│        │ = │   │
│         └───┘          └──┘        │ B │   │
│  IoU = 0.0         IoU = 0.3-0.7   │   │   │
│                                    └───┘   │
│                                  IoU = 1.0 │
└─────────────────────────────────────────────┘
```

---

### Confidence Threshold Effect

```
┌──────────────────────────────────────────────────┐
│      Confidence Threshold Impact                  │
└──────────────────────────────────────────────────┘

Original Detections (100 boxes):
┌────────────────────────────────────────┐
│ Confidence Distribution:               │
│                                        │
│  Count                                 │
│   30│     ██                           │
│   25│     ██  ██                       │
│   20│     ██  ██  ██                   │
│   15│ ██  ██  ██  ██  ██               │
│   10│ ██  ██  ██  ██  ██  ██           │
│    5│ ██  ██  ██  ██  ██  ██  ██       │
│    0└──┴───┴───┴───┴───┴───┴───┴─▶    │
│     0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8    │
│              Confidence                 │
└────────────────────────────────────────┘

Effect of Different Thresholds:

┌─────────────────────────────────────────────────┐
│ Threshold = 0.3 (Low)                           │
├─────────────────────────────────────────────────┤
│  ✓ High Recall (많은 객체 탐지)                  │
│  ✗ Low Precision (많은 False Positives)         │
│  Result: 85 boxes kept                          │
│                                                 │
│  ┌─────────────────────────────────┐            │
│  │ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐  │            │
│  │ │ ✓ │ │ ✓ │ │ ✓ │ │ ✗ │ │ ✗ │  │            │
│  │ └───┘ └───┘ └───┘ └───┘ └───┘  │            │
│  │ (Too many false detections)     │            │
│  └─────────────────────────────────┘            │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│ Threshold = 0.5 (Medium) ← Recommended          │
├─────────────────────────────────────────────────┤
│  ✓ Balanced Recall & Precision                  │
│  Result: 45 boxes kept                          │
│                                                 │
│  ┌─────────────────────────────────┐            │
│  │ ┌───┐ ┌───┐ ┌───┐               │            │
│  │ │ ✓ │ │ ✓ │ │ ✓ │               │            │
│  │ └───┘ └───┘ └───┘               │            │
│  │ (Good balance)                   │            │
│  └─────────────────────────────────┘            │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│ Threshold = 0.7 (High)                          │
├─────────────────────────────────────────────────┤
│  ✓ High Precision (적은 False Positives)        │
│  ✗ Low Recall (객체 누락 가능)                   │
│  Result: 15 boxes kept                          │
│                                                 │
│  ┌─────────────────────────────────┐            │
│  │ ┌───┐                             │            │
│  │ │ ✓ │    (missing objects)        │            │
│  │ └───┘                             │            │
│  │ (Too few detections)              │            │
│  └─────────────────────────────────┘            │
└─────────────────────────────────────────────────┘

Precision-Recall Trade-off:
┌────────────────────────────────────┐
│ Precision                          │
│   1.0 │    ╱───────                │
│       │   ╱                        │
│   0.8 │  ╱                         │
│       │ ╱                          │
│   0.6 │╱                           │
│       │                            │
│   0.4 └──────────────▶             │
│      0.0  0.4  0.8  1.0            │
│              Recall                │
│                                    │
│  • Low threshold: high recall      │
│  • High threshold: high precision  │
└────────────────────────────────────┘
```

---

### Training Loop

```
┌──────────────────────────────────────────────────┐
│           Model Training Pipeline                 │
└──────────────────────────────────────────────────┘

Epoch Loop (반복):
┌────────────────────────────────────────────────┐
│ For epoch in range(num_epochs):                │
│   │                                            │
│   ▼                                            │
│ ┌──────────────────────────────────────────┐  │
│ │         Batch Loop (미니배치)              │  │
│ │ For batch in dataloader:                  │  │
│ │   │                                       │  │
│ │   ▼                                       │  │
│ │ ┌───────────────────────────────────┐    │  │
│ │ │ 1. Load Data (데이터 로드)         │    │  │
│ │ │                                   │    │  │
│ │ │   Images ──┐                      │    │  │
│ │ │   Labels ──┤─▶ GPU/CPU            │    │  │
│ │ │   (Batch)  │                      │    │  │
│ │ └───────────┼───────────────────────┘    │  │
│ │             │                            │  │
│ │             ▼                            │  │
│ │ ┌───────────────────────────────────┐    │  │
│ │ │ 2. Forward Pass (순전파)           │    │  │
│ │ │                                   │    │  │
│ │ │   Images ──▶ Model ──▶ Predictions│    │  │
│ │ │                  │                │    │  │
│ │ │                  ▼                │    │  │
│ │ │          [Backbone]               │    │  │
│ │ │               │                   │    │  │
│ │ │          [Neck]                   │    │  │
│ │ │               │                   │    │  │
│ │ │          [Head]                   │    │  │
│ │ │               │                   │    │  │
│ │ │          Outputs                  │    │  │
│ │ └───────────┼───────────────────────┘    │  │
│ │             │                            │  │
│ │             ▼                            │  │
│ │ ┌───────────────────────────────────┐    │  │
│ │ │ 3. Compute Loss (손실 계산)        │    │  │
│ │ │                                   │    │  │
│ │ │   Predictions ──┐                 │    │  │
│ │ │   Labels ───────┤─▶ Loss Function │    │  │
│ │ │                 │                 │    │  │
│ │ │   Loss = ∑ Components:            │    │  │
│ │ │   • Box Loss (좌표 오차)           │    │  │
│ │ │   • Class Loss (분류 오차)         │    │  │
│ │ │   • Object Loss (객체 신뢰도)      │    │  │
│ │ │                                   │    │  │
│ │ │   Total = λ₁L_box + λ₂L_cls +    │    │  │
│ │ │           λ₃L_obj                 │    │  │
│ │ └───────────┼───────────────────────┘    │  │
│ │             │                            │  │
│ │             ▼                            │  │
│ │ ┌───────────────────────────────────┐    │  │
│ │ │ 4. Backward Pass (역전파)          │    │  │
│ │ │                                   │    │  │
│ │ │   optimizer.zero_grad()           │    │  │
│ │ │   loss.backward()                 │    │  │
│ │ │           │                       │    │  │
│ │ │           ▼                       │    │  │
│ │ │   Compute ∂L/∂W for all weights   │    │  │
│ │ │   (Gradients via Chain Rule)     │    │  │
│ │ │                                   │    │  │
│ │ │   Output ◄── Head ◄── Neck ◄──   │    │  │
│ │ │                        Backbone   │    │  │
│ │ │      (Gradient flow)              │    │  │
│ │ └───────────┼───────────────────────┘    │  │
│ │             │                            │  │
│ │             ▼                            │  │
│ │ ┌───────────────────────────────────┐    │  │
│ │ │ 5. Update Weights (가중치 업데이트) │    │  │
│ │ │                                   │    │  │
│ │ │   optimizer.step()                │    │  │
│ │ │           │                       │    │  │
│ │ │           ▼                       │    │  │
│ │ │   W_new = W_old - lr × ∂L/∂W     │    │  │
│ │ │                                   │    │  │
│ │ │   (Adam/SGD/AdamW optimizer)      │    │  │
│ │ └───────────────────────────────────┘    │  │
│ │                                          │  │
│ └──────────────────────────────────────────┘  │
│                                                │
│   ▼                                            │
│ ┌──────────────────────────────────────────┐  │
│ │   Validation (검증)                       │  │
│ │   • No gradient computation               │  │
│ │   • Evaluate metrics (mAP, etc.)          │  │
│ │   • Early stopping check                  │  │
│ └──────────────────────────────────────────┘  │
│                                                │
│   ▼                                            │
│ ┌──────────────────────────────────────────┐  │
│ │   Save Checkpoint (체크포인트 저장)        │  │
│ │   if val_loss < best_loss:                │  │
│ │       save_model(f"best_epoch{epoch}.pt") │  │
│ └──────────────────────────────────────────┘  │
└────────────────────────────────────────────────┘

Learning Rate Scheduling (학습률 스케줄링):
┌────────────────────────────────────┐
│ LR                                 │
│  │     Warmup      Decay           │
│  │      ╱────╲                     │
│  │     ╱      ╲___                 │
│  │    ╱           ╲___             │
│  │   ╱                ╲___         │
│  │  ╱                     ╲___     │
│  │ ╱                          ╲___ │
│  └──────────────────────────────▶  │
│  0    10    20    30    40    50   │
│              Epoch                 │
└────────────────────────────────────┘
```

---

### Inference Pipeline

```
┌──────────────────────────────────────────────────┐
│         Complete Inference Pipeline               │
└──────────────────────────────────────────────────┘

Raw Input Image
      │
      ▼
┌──────────────────────────────────────────────────┐
│           1. Preprocessing (전처리)               │
│                                                   │
│  ┌─────────────────────────────────────────┐    │
│  │ • Read image (cv2.imread/PIL)           │    │
│  │ • Convert color space (BGR→RGB)         │    │
│  │ • Resize to model input size            │    │
│  │   - Letterbox (keep aspect ratio)       │    │
│  │   - Padding to square                   │    │
│  │ • Normalize (0-255 → 0-1)               │    │
│  │ • Transpose (HWC → CHW)                 │    │
│  │ • Add batch dimension (CHW → BCHW)      │    │
│  │ • Convert to tensor (numpy → torch)     │    │
│  └─────────────────────────────────────────┘    │
│                                                   │
│  Before:                   After:                │
│  ┌───────────────┐         ┌───────────┐        │
│  │  1920×1080    │    ─▶   │  640×640  │        │
│  │  (H,W,C)      │         │  (1,3,H,W)│        │
│  │  uint8        │         │  float32  │        │
│  └───────────────┘         └───────────┘        │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│           2. Model Inference (추론)               │
│                                                   │
│  ┌─────────────────────────────────────────┐    │
│  │  with torch.no_grad():                  │    │
│  │      predictions = model(input_tensor)  │    │
│  │                                         │    │
│  │  • Forward pass only (no gradients)     │    │
│  │  • Output shape: [batch, num_pred, 85] │    │
│  │    - 4: box coords (x,y,w,h)            │    │
│  │    - 1: objectness score                │    │
│  │    - 80: class probabilities (COCO)     │    │
│  └─────────────────────────────────────────┘    │
│                                                   │
│  Raw Predictions (thousands):                    │
│  ┌───────────────────────────────────────┐      │
│  │ [0.523, 0.687, 0.124, 0.089, 0.92,... ]│      │
│  │ [0.891, 0.234, 0.456, 0.321, 0.15,... ]│      │
│  │ [0.123, 0.999, 0.234, 0.112, 0.88,... ]│      │
│  │              ... (25200 rows)            │      │
│  └───────────────────────────────────────┘      │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│         3. Postprocessing (후처리)                │
│                                                   │
│  Step 3.1: Confidence Filtering                  │
│  ┌─────────────────────────────────────────┐    │
│  │  Keep only predictions with:            │    │
│  │  conf_score > threshold (e.g. 0.5)      │    │
│  │                                         │    │
│  │  25200 predictions → ~100 candidates    │    │
│  └─────────────────────────────────────────┘    │
│                   │                              │
│                   ▼                              │
│  Step 3.2: Coordinate Conversion                 │
│  ┌─────────────────────────────────────────┐    │
│  │  • Convert (cx,cy,w,h) → (x1,y1,x2,y2)  │    │
│  │  • Scale back to original image size    │    │
│  │  • Remove padding (letterbox inverse)   │    │
│  └─────────────────────────────────────────┘    │
│                   │                              │
│                   ▼                              │
│  Step 3.3: NMS (Non-Maximum Suppression)         │
│  ┌─────────────────────────────────────────┐    │
│  │  • Group by class                       │    │
│  │  • Remove overlapping boxes             │    │
│  │  • IoU threshold (e.g. 0.45)            │    │
│  │                                         │    │
│  │  ~100 candidates → 5-20 final boxes     │    │
│  └─────────────────────────────────────────┘    │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│         4. Visualization (시각화)                 │
│                                                   │
│  ┌─────────────────────────────────────────┐    │
│  │  For each detection:                    │    │
│  │    • Draw bounding box                  │    │
│  │    • Add label text                     │    │
│  │    • Display confidence score           │    │
│  │    • Color code by class                │    │
│  └─────────────────────────────────────────┘    │
│                                                   │
│  Final Output:                                   │
│  ┌─────────────────────────────────────────┐    │
│  │                                         │    │
│  │   ┌──────────────────┐                 │    │
│  │   │ Person 0.95      │                 │    │
│  │   └──────────────────┘                 │    │
│  │          ┌─────────────┐               │    │
│  │          │ Dog 0.88    │               │    │
│  │          └─────────────┘               │    │
│  │                 ┌──────────┐           │    │
│  │                 │ Cat 0.82 │           │    │
│  │                 └──────────┘           │    │
│  │                                         │    │
│  └─────────────────────────────────────────┘    │
└──────────────────────────────────────────────────┘

Performance Optimization (성능 최적화):
┌────────────────────────────────────────────┐
│  • Batch processing (여러 이미지 동시 처리) │
│  • Mixed precision (FP16)                  │
│  • TensorRT/ONNX export                    │
│  • Model pruning/quantization              │
│  • Multi-threading for pre/post processing │
└────────────────────────────────────────────┘
```

---

## Data Flow Diagrams

### Roboflow Integration Flow

```
┌──────────────────────────────────────────────────┐
│         Roboflow Dataset Integration              │
└──────────────────────────────────────────────────┘

User Workflow:
┌────────────────────┐
│ 1. Create Project  │
│    on Roboflow     │
└─────────┬──────────┘
          │
          ▼
┌────────────────────┐
│ 2. Upload Images   │
│    & Annotate      │
│    (bbox/polygon)  │
└─────────┬──────────┘
          │
          ▼
┌────────────────────┐
│ 3. Augmentation    │
│    & Preprocessing │
│    (optional)      │
└─────────┬──────────┘
          │
          ▼
┌────────────────────┐
│ 4. Generate        │
│    Dataset Version │
└─────────┬──────────┘
          │
          ▼
┌────────────────────┐
│ 5. Export Format   │
│    (YOLO/COCO/VOC) │
└─────────┬──────────┘
          │
          ▼

Python Code Integration:
┌──────────────────────────────────────────────────┐
│  from roboflow import Roboflow                    │
│                                                   │
│  # 1. Initialize                                 │
│  rf = Roboflow(api_key="YOUR_API_KEY")           │
│      │                                            │
│      ▼                                            │
│  # 2. Get Project                                │
│  project = rf.workspace("ws").project("proj")    │
│      │                                            │
│      ▼                                            │
│  # 3. Download Dataset                           │
│  dataset = project.version(1).download("yolov8") │
│      │                                            │
│      ▼                                            │
│  ┌──────────────────────────────────────────┐   │
│  │        Local Directory Structure          │   │
│  │                                          │   │
│  │  dataset_name/                           │   │
│  │  ├── train/                              │   │
│  │  │   ├── images/                         │   │
│  │  │   └── labels/                         │   │
│  │  ├── valid/                              │   │
│  │  │   ├── images/                         │   │
│  │  │   └── labels/                         │   │
│  │  ├── test/                               │   │
│  │  │   ├── images/                         │   │
│  │  │   └── labels/                         │   │
│  │  └── data.yaml                           │   │
│  └──────────────────────────────────────────┘   │
│      │                                            │
│      ▼                                            │
│  # 4. Load in Training                           │
│  from ultralytics import YOLO                    │
│  model = YOLO("yolov8n.pt")                      │
│  model.train(data="dataset_name/data.yaml")      │
└──────────────────────────────────────────────────┘

Data Flow:
┌────────────┐     ┌────────────┐     ┌───────────┐
│  Roboflow  │────▶│  Download  │────▶│  Local    │
│  Server    │ API │  (ZIP)     │     │  Storage  │
└────────────┘     └────────────┘     └─────┬─────┘
                                             │
                   ┌─────────────────────────┘
                   │
                   ▼
         ┌──────────────────┐
         │   DataLoader     │
         │   (PyTorch)      │
         └────────┬─────────┘
                  │
                  ▼
         ┌──────────────────┐
         │  Training Loop   │
         └──────────────────┘
```

---

### supervision Visualization Pipeline

```
┌──────────────────────────────────────────────────┐
│      supervision Library Visualization Flow       │
└──────────────────────────────────────────────────┘

Model Predictions → supervision → Annotated Output

Step-by-Step Flow:
┌──────────────────────────────────────────────────┐
│ 1. Get Model Predictions                          │
│                                                   │
│  results = model(image)                           │
│                                                   │
│  Output format (YOLO):                            │
│  ┌────────────────────────────────────────┐      │
│  │ boxes: [[x1,y1,x2,y2], ...]            │      │
│  │ confidences: [0.95, 0.82, ...]         │      │
│  │ class_ids: [0, 15, 16, ...]            │      │
│  └────────────────────────────────────────┘      │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│ 2. Convert to supervision Detections              │
│                                                   │
│  import supervision as sv                         │
│                                                   │
│  detections = sv.Detections(                      │
│      xyxy=boxes,                                  │
│      confidence=confidences,                      │
│      class_id=class_ids                           │
│  )                                                │
│                                                   │
│  Detections object structure:                     │
│  ┌────────────────────────────────────────┐      │
│  │ • xyxy: np.ndarray [N, 4]              │      │
│  │ • confidence: np.ndarray [N]           │      │
│  │ • class_id: np.ndarray [N]             │      │
│  │ • tracker_id: Optional[np.ndarray]     │      │
│  │ • data: Dict (custom metadata)         │      │
│  └────────────────────────────────────────┘      │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│ 3. Create Annotators (주석 도구 생성)             │
│                                                   │
│  # Bounding box annotator                        │
│  box_annotator = sv.BoxAnnotator(                │
│      thickness=2,                                │
│      color=sv.ColorPalette.DEFAULT              │
│  )                                                │
│                                                   │
│  # Label annotator                               │
│  label_annotator = sv.LabelAnnotator(            │
│      text_position=sv.Position.TOP_LEFT,        │
│      text_padding=5                              │
│  )                                                │
│                                                   │
│  # Additional annotators                         │
│  trace_annotator = sv.TraceAnnotator()           │
│  halo_annotator = sv.HaloAnnotator()             │
│  mask_annotator = sv.MaskAnnotator()             │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│ 4. Generate Labels (라벨 생성)                    │
│                                                   │
│  labels = [                                       │
│      f"{class_names[class_id]} {conf:0.2f}"      │
│      for class_id, conf                          │
│      in zip(detections.class_id,                 │
│              detections.confidence)              │
│  ]                                                │
│                                                   │
│  Example output:                                 │
│  ["person 0.95", "dog 0.88", "cat 0.82"]         │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│ 5. Annotate Image (이미지에 주석 추가)            │
│                                                   │
│  annotated = image.copy()                        │
│  annotated = box_annotator.annotate(             │
│      scene=annotated,                            │
│      detections=detections                       │
│  )                                                │
│  annotated = label_annotator.annotate(           │
│      scene=annotated,                            │
│      detections=detections,                      │
│      labels=labels                               │
│  )                                                │
│                                                   │
│  Visual transformation:                          │
│  ┌──────────────┐         ┌─────────────────┐   │
│  │              │         │ ┌────────────┐  │   │
│  │   Original   │    ─▶   │ │Person 0.95 │  │   │
│  │   Image      │         │ └────────────┘  │   │
│  │              │         │    ┌─────────┐  │   │
│  │              │         │    │Dog 0.88 │  │   │
│  └──────────────┘         │    └─────────┘  │   │
│                           └─────────────────┘   │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────┐
│ 6. Advanced Features (고급 기능)                  │
│                                                   │
│  # Object tracking (객체 추적)                    │
│  tracker = sv.ByteTrack()                        │
│  detections = tracker.update_with_detections(    │
│      detections                                  │
│  )                                                │
│                                                   │
│  # Filtering (필터링)                             │
│  detections = detections[                        │
│      detections.confidence > 0.5                 │
│  ]                                                │
│                                                   │
│  # Zones (구역 모니터링)                          │
│  zone = sv.PolygonZone(                          │
│      polygon=np.array([[0,0], [100,0], ...])    │
│  )                                                │
│  zone.trigger(detections)                        │
│                                                   │
│  # Line crossing (라인 교차 카운팅)               │
│  line_counter = sv.LineZone(                     │
│      start=sv.Point(0, 500),                    │
│      end=sv.Point(1920, 500)                    │
│  )                                                │
│  line_counter.trigger(detections)                │
└──────────────────────────────────────────────────┘

Common Annotator Options:
┌─────────────────────────────────────────────────┐
│ BoxAnnotator:                                   │
│  • thickness: line width                        │
│  • color: ColorPalette or custom                │
│  • color_lookup: COLOR_BY_CLASS/TRACK/INDEX     │
│                                                 │
│ LabelAnnotator:                                 │
│  • text_position: TOP_LEFT/CENTER/BOTTOM_RIGHT  │
│  • text_padding: padding around text            │
│  • text_scale: font size                        │
│  • text_thickness: font weight                  │
│                                                 │
│ MaskAnnotator:                                  │
│  • opacity: mask transparency (0-1)             │
│  • color: mask fill color                       │
│                                                 │
│ TraceAnnotator:                                 │
│  • trace_length: number of points to show       │
│  • thickness: line thickness                    │
└─────────────────────────────────────────────────┘
```

---

## Summary Table

### Model Comparison

| Model | Task | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| YOLO | Detection | 5/5 | 3/5 | 실시간 객체 탐지 |
| RT-DETR | Detection | 4/5 | 4/5 | 고정밀 탐지 |
| SAM | Segmentation | 2/5 | 5/5 | 범용 분할 |
| SAM 2 | Video Seg | 3/5 | 4/5 | 비디오 분할 |
| YOLO-Seg | Instance Seg | 5/5 | 3/5 | 실시간 인스턴스 분할 |
| Florence-2 | VLM | 3/5 | 4/5 | 멀티태스크 비전 |
| Qwen2.5-VL | VLM | 2/5 | 5/5 | 고성능 VQA |
| PaliGemma | VLM | 3/5 | 3/5 | 경량 VLM |

### Architecture Components

| Component | Purpose | Key Techniques |
|-----------|---------|----------------|
| Backbone | Feature extraction | CNN, ViT, CSPDarknet |
| Neck | Feature fusion | FPN, PANet |
| Head | Task prediction | Detection, Segmentation |
| Encoder | Image understanding | Self-attention |
| Decoder | Output generation | Cross-attention |
| Projector | Modality bridging | Linear, MLP |

---

## References

이 다이어그램들은 다음 논문과 공식 문서를 참고하여 작성되었습니다:

- YOLOv8/v11: Ultralytics documentation
- RT-DETR: "DETRs Beat YOLOs on Real-time Object Detection" (arXiv:2304.08069)
- SAM: "Segment Anything" (arXiv:2304.02643)
- SAM 2: "SAM 2: Segment Anything in Images and Videos" (arXiv:2408.00714)
- Florence-2: "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" (arXiv:2311.06242)
- Qwen2.5-VL: Qwen official documentation
- PaliGemma: Google Research blog
- supervision: Roboflow supervision library documentation

---

**사용 방법:**
이 다이어그램들은 Jupyter 노트북의 마크다운 셀에 복사하여 사용하거나,
터미널에서 직접 볼 수 있도록 설계되었습니다. 각 다이어그램은 모델의
핵심 구조와 동작 원리를 시각적으로 이해할 수 있도록 구성되었습니다.
